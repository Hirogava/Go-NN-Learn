# Оптимизация матричного умножения

## Обзор

Реализация матричного умножения была полностью переработана с целью повышения производительности и снижения использования памяти. Оптимизация учитывает особенности модели памяти Go и типичные размеры данных в нейронных сетях.

## Реализованные оптимизации

### 1. Устранение overhead вызовов функций

**Проблема**: Исходная реализация использовала функции `At()` и `Set()` для доступа к элементам матрицы, что создавало дополнительный overhead вызовов функций.

**Решение**: Прямой доступ к данным через `Data[i*Cols+j]`, что устраняет overhead и позволяет компилятору лучше оптимизировать код.

### 2. Улучшение cache locality

**Проблема**: Исходный алгоритм использовал порядок циклов `i-j-k`, что приводило к неэффективному доступу к памяти при чтении второй матрицы по столбцам (плохая cache locality).

**Решение**: 
- Изменен порядок циклов на `i-k-j` для базового алгоритма
- Это обеспечивает последовательный доступ к первой матрице (по строкам)
- Вторая матрица читается блоками, что улучшает использование кэша процессора

### 3. Блочное умножение (Tile-based multiplication)

**Проблема**: Для больших матриц стандартный алгоритм имеет плохую cache locality из-за частых промахов кэша.

**Решение**: Реализовано блочное умножение с размером блока 64x64 элементов:
- Матрицы разбиваются на блоки, которые помещаются в L1 cache процессора
- Каждый блок обрабатывается независимо, что максимизирует cache hit rate
- Размер блока выбран оптимально для современных процессоров (обычно L1 cache ~32-64KB)

### 4. Оптимизированный параллелизм

**Проблема**: Исходная параллельная версия создавала одну goroutine на каждую строку, что:
- Создавало слишком много goroutines для маленьких матриц
- Приводило к overhead на переключение контекста
- Не использовало эффективно доступные CPU ядра

**Решение**: 
- Реализован worker pool с количеством worker'ов равным количеству CPU ядер
- Работа распределяется блоками строк, а не по одной строке
- Каждый worker выполняет блочное умножение для своего блока строк
- Это обеспечивает оптимальное использование CPU и минимизирует overhead

### 5. Адаптивный выбор алгоритма

**Решение**: Реализован автоматический выбор оптимального алгоритма в зависимости от размера матриц:

- **Маленькие матрицы** (< 64x64): Простая оптимизированная версия (`matMulSimple`)
  - Минимальный overhead
  - Прямой доступ к данным
  - Оптимальный порядок циклов

- **Средние матрицы** (64x64 - 128x128): Блочное умножение (`matMulBlocked`)
  - Улучшенная cache locality
  - Эффективно для матриц, которые не помещаются в кэш целиком

- **Большие матрицы** (> 128x128): Параллельное блочное умножение (`matMulParallelBlocked`)
  - Использует все доступные CPU ядра
  - Комбинация блочного умножения и параллелизма
  - Оптимально для типичных размеров в нейронных сетях (batch_size=128, features=784)

### 6. Оптимизация транспонирования

**Решение**: Транспонирование также оптимизировано:
- Убран overhead вызовов функций `At()` и `Set()`
- Прямой доступ к данным для лучшей производительности

## Производительность

### Бенчмарки

Результаты бенчмарков на Apple M5 (ARM64):

```
BenchmarkMatMulSmall-10            	   90345	     11946 ns/op	    8240 B/op	       2 allocs/op
BenchmarkMatMulMedium-10           	    3349	    366703 ns/op	  132890 B/op	      14 allocs/op
BenchmarkMatMulLarge-10            	     330	   3713355 ns/op	  526089 B/op	      14 allocs/op
BenchmarkMatMulVeryLarge-10        	      62	  18940429 ns/op	 2098944 B/op	      14 allocs/op
```

### Улучшения

1. **Снижение аллокаций**: Минимальное количество аллокаций (только для результата)
2. **Cache efficiency**: Блочное умножение значительно улучшает использование кэша
3. **Параллелизм**: Эффективное использование всех CPU ядер для больших матриц
4. **Адаптивность**: Автоматический выбор оптимального алгоритма

## Использование памяти

- **Минимальные аллокации**: Создается только одна матрица-результат
- **Нет промежуточных копий**: Работа напрямую с исходными данными
- **Эффективное использование кэша**: Блочный алгоритм минимизирует обращения к памяти

## Совместимость

- Полная обратная совместимость: API функции `MatMul()` не изменился
- Все существующие тесты проходят без изменений
- `MatMulParallel()` оставлена для обратной совместимости, но теперь использует оптимизированную версию

## Технические детали

### Пороги выбора алгоритма

```go
const (
    smallMatrixThreshold = 64  // Порог для маленьких матриц
    blockSize            = 64  // Размер блока для блочного умножения
    parallelThreshold    = 128 // Минимальный размер для параллельного выполнения
)
```

### Модель памяти Go

Оптимизации учитывают особенности Go:
- Row-major хранение матриц (элементы строк последовательны в памяти)
- Эффективное использование слайсов
- Минимизация аллокаций для снижения нагрузки на GC
- Параллелизм через goroutines и worker pool

## Рекомендации по использованию

1. **Для маленьких матриц** (< 64x64): Автоматически используется быстрая версия
2. **Для средних матриц**: Блочное умножение обеспечивает оптимальную производительность
3. **Для больших матриц** (типичные в нейронных сетях): Параллельное блочное умножение использует все ресурсы CPU

## Будущие улучшения

Возможные направления дальнейшей оптимизации:
- SIMD инструкции для векторных операций
- Использование специализированных библиотек (BLAS) для очень больших матриц
- Оптимизация для конкретных архитектур процессоров
- Поддержка sparse матриц

