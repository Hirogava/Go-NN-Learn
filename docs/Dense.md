# Создание пользовательских слоев в Go-NN-Learn

## Обзор

Это руководство объясняет процесс разработки собственных слоев нейронной сети в библиотеке Go-NN-Learn. На примере
реализации Dense-слоя рассматриваются ключевые аспекты: регистрация параметров, прямой проход и интеграция с системой
автоматического дифференцирования.

## Быстрый старт

### Требуемые интерфейсы

```go
type Layer interface {
    Forward(x *graph.Node) *graph.Node
    Params() []*graph.Node
}
```

## Шаблон пользовательского слоя

- ### Определите структуру:

```go
type CustomLayer struct {
    params []*graph.Node
    // дополнительные поля
}
```

- ### Реализуйте конструктор:

```go
func NewCustomLayer(args) *CustomLayer {
    // Инициализация параметров как graph.Node
}
```

- ### Реализуйте Forward:

```go
func (l *CustomLayer) Forward(x *graph.Node) *graph.Node {
    // Вычисления и создание выходного узла
    // Прикрепите операцию для backward
}
```

- ### Реализуйте Params:

```go
func (l *CustomLayer) Params() []*graph.Node {
    return l.params
}
```

- ### Определите операцию для autograd:

```go
type customOp struct {
    // ссылки на входные узлы
}

func (op *customOp) Backward(grad *tensor.Tensor) {
    // вычисление градиентов
}
```

## Пример: Dense Layer

### Структура

```go
type Dense struct {
    weights *graph.Node
    bias    *graph.Node
    inDim   int
    outDim  int
}
```

### Ключевые особенности

- Параметры как узлы графа: weights и bias хранятся как *graph.Node
- Инициализация: Конструктор принимает функцию инициализации параметров
- Прямой проход: Матричное умножение с добавлением смещения
- Автоград: Прикрепление операции denseOp к выходному узлу

## Система автоматического дифференцирования

### Принцип работы

- Прямой проход: Каждый слой создает узлы графа с операциями
- Построение графа: Узлы связываются через родительские отношения
- Обратное распространение: Вызов Backward для каждой операции в обратном порядке
- Накопление градиентов: Градиенты сохраняются в поле Grad каждого узла

## Интеграция слоя

### Для поддержки автограда:

- Прикрепите операцию к выходному узлу в Forward
- Реализуйте метод Backward для вычисления градиентов
- Убедитесь, что градиенты имеют правильные формы

## Лучшие практики

### Проектирование

- Все обучаемые параметры должны быть *graph.Node
- Проверяйте размерности входных данных
- Документируйте ожидаемые форматы тензоров
- ### Производительность
- Минимизируйте аллокации в Forward
- Используйте пулы тензоров для повторного использования
- Оптимизируйте матричные операции
- ### Тестирование
- Проверяйте градиенты с помощью CheckGradientEngine
- Тестируйте на различных размерностях входа
- Убедитесь в правильности shapes на всех этапах

## Расширенные сценарии

### Составные слои

Создавайте сложные слои, комбинируя простые:

```go
type MLPBlock struct {
    dense *Dense
    norm  *BatchNorm
    activation *ReLU
}
```

### Условное выполнение

Реализуйте слои с разным поведением в обучении и inference:

```go
func (l *CustomLayer) Forward(x *graph.Node) *graph.Node {
    if l.training {
        // обучение
    } else {
        // inference
    }
}
```